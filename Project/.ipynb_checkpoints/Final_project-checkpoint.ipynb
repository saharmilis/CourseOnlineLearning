{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Streaming Algorithms and Online Learning\n",
    "## Final Project\n",
    "By Millis Sahar  \n",
    "ID 300420379"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General\n",
    "As mentioned in class, most importantly, the ideal project should be interesting.  \n",
    "Generally, it should focus on some “small” research topic (see suggested topics below, or\n",
    "your own topic), and include the following parts:  \n",
    "1. Your summary of what’s known about the topic, based on relevant literature. You should follow strict academic writing guidelines, including proper citation and clarity.  \n",
    "2. Experimental part, which consists of implementation and validation on data (either real or simulated). This part can serve as an empirical confirmation (or refutation) of a known theoretical result or conjecture.  \n",
    "\n",
    "Projects do not must to have both parts, but they should require (and will be evaluated accordingly) a proper amount of effort.\n",
    "\n",
    "<br>\n",
    "\n",
    "For example:\n",
    "- if you write a comprehensive survey of a topic that may become the ”standard text” that people read for that topic, it’s great.  \n",
    "- If you make interesting progress in an open question, even “small one”, it’s great.  \n",
    "- If you write a short summary but implement a huge, large scale project withinteresting empirical findings, it’s great.  \n",
    "\n",
    "You don’t have to decide in advance whether you’re going to spend more time on the theoretical or on the experimental side.   \n",
    "I suggest you start reading about a topic you find interesting, think about it, and see where it takes you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sublinear Algorithms\n",
    "Sublinear algorithms include any algorithm that runs in time o(n) for input of size n.  \n",
    "Often, you don’t even see the entire input. \n",
    "There is a nice list of open problems in sublinear algorithms (and some of these problems involve streaming and dimensionality reduction):  \n",
    "https://sublinear.info/index.php?title=Open_Problems:By_Number\n",
    "It’s a great place to get ideas, together with relevant literature.\n",
    "\n",
    "#####  Clustering\n",
    "Clustering means to partition all data points such that each partition consists of “similar” or “close” points, while all those between two clusters are “different” or “far”.   \n",
    "In MachineLearning, clustering is considered the most important examples of unsupervised learning.  \n",
    "There are extensive previous work on clustering, for example [6,34] are some interesting results related k-means.   \n",
    "Example of literature on streaming algorithms for clustering data can be found in [1,4,16].  \n",
    "\n",
    "##### Nearest neighbor search\n",
    "Nearest neighbor searching is aiming at preprocess a dataset so that, given a new data point, the closest datapoint can be found (with high probability). \n",
    "It is widely used in MachineLearning as a non-parametric learning algorithm (given a new instance point, assign to it the\n",
    "label of the closest instance you had in training set).  \n",
    "For theoretical papers, you can find some upper bounds in [23,33,39], and some lower bounds in [35, 36].  \n",
    "There is also much practical work in nearest neighbor searching, see [32].  \n",
    "\n",
    "##### Sparse Recovery\n",
    "Sparse recovery is the idea that, if you have an (approximate) sparse prior on your data, then you need only few (random) measurements in order to faithfully represent it.   \n",
    "The theory of applications of sparse recovery (as known as “compressed sensing”) is deep also from a practical point of view.   [13] is the classis intro to this field, [44] discusses the connection with random projections.   \n",
    "Some more recent stuff are [3,24].  \n",
    "\n",
    "##### MapReduce\n",
    "This large scale distributed computational architecture is widely used. See [21, 27, 30, 29,43].  \n",
    "Matrix Sampling and Sketching, Large Scale Numerical Linear Algebra How to “summarize” large matrices, and what operations can we estimate using this summarization? See [17,18,19,20,31,42].  \n",
    "Matrix Completion (The ”Netflix” Problem) The idea of matrix completion is how to approximately complete a big matrix when only\n",
    "small part of it is known. \n",
    "This subject has much work in both the practical and theoretical extremes. \n",
    "In [9,10,28] Yehuda Koren’s team explain how they won the Netflix challenge.  \n",
    "There is also vast literature in practical Machine-Learning [14, 15, 22, 40].  \n",
    "\n",
    "##### Random Projections\n",
    "Random projections is about reducing dimensionality of data that is oblivious to the data.  \n",
    "For Euclidean metrics, a name that is almost synonymous with “random projections” is “JohnsonLindenstrauss” [25]. See also [2,5,26].  \n",
    "Random projections have been also studied as preprocessing steps for other MachineLearning algorithms that rely on input dimensionality (SVM, linear regression, SVD, etc). See [7,37,38].  \n",
    "\n",
    "##### Conferences:\n",
    "You can also search the prestigious Big-data conferences:  \n",
    "- Theoretical focus:  \n",
    "    - FOCS. Foundations of Computer Science.  \n",
    "    - ICALP (track A). International Colloquium on Automata, Languages and Programming.\n",
    "    - PODS. Symposium on Principles of Database Systems.\n",
    "    - RANDOM. International Workshop on Randomization and Computation.\n",
    "    - SODA. Symposium on Discrete Algorithms.\n",
    "    - STOC. Symposium on the Theory of Computing.  \n",
    "<br>\n",
    "- Experimental focus:\n",
    "    - KDD. Knowledge Discovery and Data Mining. (data mining)\n",
    "    - ICML. International Conference on Machine Learning. (machine learning)\n",
    "    - NIPS. Neural Information Processing Systems. (machine learning)\n",
    "    - SIGMOD. ACM SIGMOD International Conference on Management of Data. (databases)\n",
    "    - VLDB. Very Large Data Bases. (databases) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Streaming-data algorithms for high-quality clustering. In Proceedings of the 18th\n",
    "International Conference on Data Engineering, ICDE ’02, pages 685–, 2002.\n",
    "\n",
    "[2] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnsonlindenstrauss transform. In Proceedings of the thirty-eighth annual ACM symposium on\n",
    "Theory of computing, pages 557–563. ACM, 2006.\n",
    "\n",
    "[3] Nir Ailon, Yudong Chen, and Xu Huan. Breaking the small cluster barrier of graph\n",
    "clustering. arXiv preprint arXiv:1302.4549, 2013.\n",
    "\n",
    "[4] Nir Ailon, Ragesh Jaiswal, and Claire Monteleoni. Streaming k-means approximation. In\n",
    "Advances in Neural Information Processing Systems 22: 23rd Annual Conference on Neural\n",
    "Information Processing Systems 2009. Proceedings of a meeting held 7-10 December 2009,\n",
    "Vancouver, British Columbia, Canada., pages 10–18, 2009.\n",
    "\n",
    "[5] Nir Ailon and Edo Liberty. An almost optimal unrestricted fast johnson-lindenstrauss\n",
    "transform. ACM Transactions on Algorithms (TALG), 9(3):21, 2013.\n",
    "\n",
    "[6] David Arthur and Sergei Vassilvitskii. k-means++: The advantages of careful seeding. In\n",
    "Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, pages\n",
    "1027–1035. Society for Industrial and Applied Mathematics, 2007.\n",
    "\n",
    "[7] Haim Avron, Petar Maymounkov, and Sivan Toledo. Blendenpik: Supercharging lapack’s\n",
    "least-squares solver. SIAM Journal on Scientific Computing, 32(3):1217–1236, 2010.\n",
    "\n",
    "[8] Nikhil Bansal, Avrim Blum, and Shuchi Chawla. Correlation clustering. Machine\n",
    "Learning, 56:89–113, 2004. \n",
    "\n",
    "[9] Robert M Bell and Yehuda Koren. Lessons from the netflix prize challenge. ACM\n",
    "SIGKDD Explorations Newsletter, 9(2):75–79, 2007.\n",
    "\n",
    "[10] Robert M Bell, Yehuda Koren, and Chris Volinsky. The bellkor solution to the netflix\n",
    "prize, 2007.\n",
    "\n",
    "[11] Francesco Bonchi, David Garc´ıa-Soriano, and Edo Liberty. Correlation clustering: from\n",
    "theory to practice. In The 20th ACM SIGKDD International Conference on Knowledge\n",
    "Discovery and Data Mining, KDD ’14, New York, NY, USA - August 24 - 27, 2014, page\n",
    "1972, 2014.\n",
    "\n",
    "[12] Christos Boutsidis, Petros Drineas, and Malik Magdon-Ismail. Near optimal\n",
    "columnbased matrix reconstruction. In FOCS, pages 305–314, 2011. 6\n",
    "\n",
    "[13] E. J. Candes, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal\n",
    "reconstruction from highly incomplete frequency information. IEEE Trans. Inf. Theor.,\n",
    "52(2):489–509, 2006.\n",
    "\n",
    "[14] Emmanuel J Candes and Benjamin Recht. Exact matrix completion via convex\n",
    "optimization. Foundations of Computational mathematics, 9(6):717–772, 2009.\n",
    "\n",
    "[15] Emmanuel J Candes and Terence Tao. The power of convex relaxation: Near-optimal\n",
    "matrix completion. Information Theory, IEEE Transactions on, 56(5):2053–2080, 2010.\n",
    "\n",
    "[16] Moses Charikar, Liadan O’Callaghan, and Rina Panigrahy. Better streaming algorithms\n",
    "for clustering problems. In Proceedings of the 35th Annual ACM Symposium on Theory of\n",
    "Computing, June 9-11, 2003, San Diego, CA, USA, pages 30–39, 2003.\n",
    "\n",
    "[17] Amit Deshpande and Santosh Vempala. Adaptive sampling and fast low-rank matrix\n",
    "approximation. In APPROX-RANDOM, pages 292–303, 2006.\n",
    "\n",
    "[18] Petros Drineas, Malik Magdon-Ismail, Michael W. Mahoney, and David P. Woodruff.\n",
    "Fast approximation of matrix coherence and statistical leverage. 2011.\n",
    "\n",
    "[19] Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for\n",
    "l2 regression and applications. In SODA, 2006.\n",
    "\n",
    "[20] Alan Frieze, Ravi Kannan, and Santosh Vempala. Fast monte-carlo algorithms for\n",
    "finding low-rank approximations. J. ACM, 51(6):1025–1041, November 2004.\n",
    "\n",
    "[21] Michael T Goodrich, Nodari Sitchinava, and Qin Zhang. Sorting, searching, and\n",
    "simulation in the mapreduce framework. In Algorithms and Computation, pages 374–383.\n",
    "Springer, 2011.\n",
    "\n",
    "[22] D. Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Trans.\n",
    "Inf. Theor., 57(3):1548–1566, March 2011.\n",
    "\n",
    "[23] Sariel Har-Peled, Piotr Indyk, and Rajeev Motwani. Approximate nearest neighbor:\n",
    "Towards removing the curse of dimensionality. Theory of computing, 8(1):321–350, 2012. \n",
    "\n",
    "[24] Jarvis Haupt, Rui M Castro, and Robert Nowak. Distilled sensing: Adaptive sampling\n",
    "for sparse detection and estimation. Information Theory, IEEE Transactions on, 57(9):6222–\n",
    "6235, 2011.\n",
    "\n",
    "[25] W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert\n",
    "space. Contemporary Mathematics, 26:189–206, 1984.\n",
    "\n",
    "[26] Daniel M Kane and Jelani Nelson. A derandomized sparse johnson-lindenstrauss\n",
    "transform. arXiv preprint arXiv:1006.3585, 2010. 7\n",
    "\n",
    "[27] Howard Karloff, Siddharth Suri, and Sergei Vassilvitskii. A model of computation for\n",
    "mapreduce. In Proceedings of the Twenty-First Annual ACM-SIAM Symposium on Discrete\n",
    "Algorithms, pages 938–948. Society for Industrial and Applied Mathematics, 2010.\n",
    "\n",
    "[28] Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for\n",
    "recommender systems. Computer, 42(8):30–37, 2009.\n",
    "\n",
    "[29] Ravi Kumar, Benjamin Moseley, Sergei Vassilvitskii, and Andrea Vattani. Fast greedy\n",
    "algorithms in mapreduce and streaming. In Proceedings of the 25th ACM symposium on\n",
    "Parallelism in algorithms and architectures, pages 1–10. ACM, 2013.\n",
    "\n",
    "[30] Silvio Lattanzi, Benjamin Moseley, Siddharth Suri, and Sergei Vassilvitskii. Filtering: a\n",
    "method for solving graph problems in map-reduce. In Proceedings of the twenty-third annual\n",
    "ACM symposium on Parallelism in algorithms and architectures, pages 85–94. ACM, 2011.\n",
    "\n",
    "[31] Edo Liberty. Simple and deterministic matrix sketching. In The 19th ACM SIGKDD\n",
    "International Conference on Knowledge Discovery and Data Mining, KDD 2013, Chicago,\n",
    "IL, USA, August 11-14, 2013, pages 581–588, 2013.\n",
    "\n",
    "[32] Ting Liu, Charles Rosenberg, and Henry A Rowley. Clustering billions of images with\n",
    "large scale nearest neighbor search. In Applications of Computer Vision, 2007. WACV’07.\n",
    "IEEE Workshop on, pages 28–28. IEEE, 2007.\n",
    "\n",
    "[33] Huy L Nguyen. Approximate nearest neighbor search. arXiv preprint arXiv:1306.3601,\n",
    "2013.\n",
    "\n",
    "[34] Rafail Ostrovsky, Yuval Rabani, Leonard J Schulman, and Chaitanya Swamy. The\n",
    "effectiveness of lloyd-type methods for the k-means problem. Journal of the ACM (JACM),\n",
    "59(6):28, 2012.\n",
    "\n",
    "[35] Ryan ODonnell, Yi Wu, and Yuan Zhou. Optimal lower bounds for locality-sensitive\n",
    "hashing (except when q is tiny). ACM Transactions on Computation Theory (TOCT), 6(1):5,\n",
    "2014.\n",
    "\n",
    "[36] Rina Panigrahy, Kunal Talwar, and Udi Wieder. Lower bounds on near neighbor search\n",
    "via metric expansion. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE\n",
    "Symposium on, pages 805–814. IEEE, 2010.\n",
    "\n",
    "[37] Saurabh Paul, Christos Boutsidis, Malik Magdon-Ismail, and Petros Drineas. Random\n",
    "projections for linear support vector machines. TKDD, 8(4):22, 2014. \n",
    "\n",
    "[38] Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. In In\n",
    "Neural Infomration Processing Systems, 2007. 8\n",
    "\n",
    "[39] Ilya Razenshteyn. Beyond Locality-Sensitive Hashing. PhD thesis, Massachusetts\n",
    "Institute of Technology, 2014.\n",
    "\n",
    "[40] Benjamin Recht. A simpler approach to matrix completion. The Journal of Machine\n",
    "Learning Research, 12:3413–3430, 2011.\n",
    "\n",
    "[41] Benjamin Recht and Christopher Re. Parallel stochastic gradient algorithms for\n",
    "largescale matrix completion. Mathematical Programming Computation, 5(2):201–226, 2013.\n",
    "\n",
    "[42] Tamas Sarlos. Improved approximation algorithms for large matrices via random\n",
    "projections. In FOCS, pages 143–152, 2006.\n",
    "\n",
    "[43] Leslie G Valiant. A bridging model for multi-core computing. In Algorithms-ESA 2008,\n",
    "pages 13–28. Springer, 2008.\n",
    "\n",
    "[44] Rachel Ward and Felix Krahmer. New and improved Johnson-Lindenstrauss\n",
    "embeddings via the restricted isometry property. SIAM Jorunal on Mathematical Analysis,\n",
    "43:1269– 1281, 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take from here:\n",
    "https://github.com/gavr97/dimensionality-reduction    \n",
    "https://github.com/rasbt/python-machine-learning-book/tree/master/faq\n",
    "https://lvdmaaten.github.io/publications/papers/TR_Dimensionality_Reduction_Review_2009.pdf\n",
    "\n",
    "https://github.com/vashistak/dimensionality-reduction-techniques\n",
    "    \n",
    "    \n",
    "non relevent:\n",
    "https://ieeexplore.ieee.org/document/6780074\n",
    "http://www.cs.ucr.edu/~eamonn/kais_2000.pdf\n",
    "https://gist.github.com/Mashimo/b8a8d4dc18bf6875c8547134b543898f\n",
    "https://gist.github.com/Mashimo/69f0972d51358d65f088a7147dfc5ff1\n",
    "https://rafalab.github.io/dsbook/large-datasets.html#dimension-reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "בהתאם להנחיות, התחלתי את הפרוייקט כאשר רציתי להתמקד בקונספט של צמצום ממדיות, כאשר היה לי דאטה סט עם מאות מימדים והמטרה הייתה להראות צורות שונות של צמצום מימדים על גבי הדאטה הקיים, בשימוש האימפלמנטציות של סייקטליירנג \n",
    "אך לאחר הרפטטיביות במשפש \"זה משעמם\" של הפרופסור שלי,\n",
    "בחרתי להפעיל שיטות של צמצום מימדיות על תמונות תלת מימדיות.\n",
    "בזמן שחיפשתי מאמרים ורפרנסים בנושא נתקלתי במאמר הזה\n",
    "Dimensionality reduction for fast and accurate video search and retrieval in a large scale database,2013\n",
    "by Utkarsha S Pacharaney ; Pritam S. Salankar ; Saradadevi Mandalapu\n",
    "https://ieeexplore.ieee.org/document/6780074\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
