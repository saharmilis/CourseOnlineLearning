{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Streaming & Online Learning - Lecture 4\n",
    "Scribing by Millis Sahar\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "### Review of Count Min Sketch\n",
    "$ Pr(\\text{Failure of d hash functions}) < \\frac{1}{e}^d \\Rightarrow \\space d=O(log\\frac{1}{\\delta}) \\space and \\space w=O(\\frac{1}{\\epsilon}) $\n",
    "\n",
    "<br>\n",
    "\n",
    "### Review $ F_2 $ Estimation\n",
    "Basic estimator : large variance   \n",
    "(similar as Flajolet Martin)\n",
    "\n",
    "Better estimator: take the median values for number of average estimators\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## K-wise Independece \n",
    "In $F_2$, recall our estimator   \n",
    "$y=X^2$  \n",
    "$Var(y) \\leq E(X^4)$   \n",
    "$X= \\sum\\limits_{a \\in Stream} h(a)f_a$\n",
    "$ In \\space order \\space to \\space simple \\space it, \\space we \\space assumed \\space 4wise \\space independence \\space$\n",
    "$and \\space then \\space used \\space that \\space E\\big(h(a)\\big)=1*0.5+(-1)*0.5=0 $\n",
    "$ Var\\big(h(a)\\big)=E\\big(h^2(a)\\big)-E^2\\big(h(a)\\big) = 1 - 0 = 1$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Count Sketch\n",
    "\n",
    "Recall Count min gives us a guarantee with respect to $F_1$\n",
    "$Pr(f_x \\leq \\widetilde{f_x} \\leq f_x + \\epsilon F_1) $\n",
    "Now, we will have a \"better\" guarantee, becuase we won't always be overestimating (supposedly is better/more accurate on averagethen Count min although both counters are \"unbalanced\")\n",
    "However, since $w=\\frac{3}{\\epsilon ^2}$ in this case, and $\\epsilon \\in [0,1] $, w is bigger then before and needs more space.\n",
    "In this case, we have $C_{ij} = \\sum\\limits_{x \\in H(x)}r_i(x)f_x$\n",
    "such that\n",
    "$r_i[n] \\Rightarrow \\{-1,1\\} $ etc.\n",
    "Take median:\n",
    "\n",
    "$$\\underbrace{r_1(a)c_1,r_2(a)c_2,r_3(a)c_3,r_4(a)c_4}_{take \\space median}$$\n",
    "\n",
    "<br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "##### Recall Trick\n",
    "$ n_1, ... ,n_k $ k estimators  \n",
    "$n_{average} = \\frac{1}{k}\\sum n_i$  \n",
    "$E(n_{average}) = \\frac{1}{k}\\sum \\limits_{i=1}^{k}Var(n_i) = \\frac{1}{k^2} k Var(\\widetilde{n_i})$  \n",
    "\n",
    "##### Median trick\n",
    "Choose the median estimator value out of n estimotors.\n",
    "<br><br>\n",
    "\n",
    "### Now we will prove two lemma's \n",
    "1. $ E\\big( r_i(x) c_{i H_i(x)} \\big) = f_x$\n",
    "2. $ Var\\big( r_i(x) c_{i H_i(x)} \\big) \\leq \\frac{F_2}{w} $\n",
    "\n",
    "##### Lemma 1 proof: \n",
    "for specific X with one hash function\n",
    "\n",
    "$C_{H(x)} = \\sum \\limits_{y \\in Stream} r(y) f_y I_{xy}$\n",
    "\n",
    "$ \\begin{equation}\n",
    "  I_{xy} =\\begin{cases}\n",
    "    1, & h(x)=h(y)\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{equation} $\n",
    "\n",
    "$E\\big( r(x) \\sum r(y)f_yI_{xy} \\big) =$  \n",
    "$ \\space \\space \\space = E\\big( \\sum \\limits_{y} r(x)r(y) f_y I_{xy} \\big) = $   \n",
    "$ \\space \\space \\space = E\\big( r^2(x) f_x I_{xx} + \\sum \\limits_{y \\neq x} r(x)r(y) f_y I_{xy}  \\big) = $  \n",
    "$ \\space \\space \\space = E\\big( 1f_x1 + \\sum \\limits_{y \\neq x} 0 * f_y I_{xy} \\big) = $   \n",
    "$ \\space \\space \\space = E\\big( f_x + 0 \\big) = E(f_x) = f_x $\n",
    "\n",
    "\n",
    "##### Lemma 2 proof: \n",
    "$ Var\\big(r(x)c_{H(x)}\\big)  \\leq E\\big( r(x)C_{H(x)} \\big) $  \n",
    "by definition of Variance   \n",
    "\n",
    "$ E\\bigg( \\big( \\sum \\limits_{y} r(x)r(y)f_yI_{xy}  \\big)   \\big( \\sum \\limits_{y'} r(x)r(y') f_{y'} I_{xy'}  \\big)  \\bigg) = $\n",
    "$ E\\bigg( \\big( \\sum \\limits_{y=y'} r^2(x)r^2(y)f^2_{y} I^2_{xy}  \\big) + \\big( \\sum \\limits_{y \\neq y'} r^2(x)r(y)r(y') f_y f_{y'} I I  \\big)  \\bigg) = $\n",
    "$ = E\\bigg( \\sum \\limits_{y=y'} 1* 1 * f^2 I^2_{xy} + \\big( \\sum \\limits_{y \\neq y'} r^2(x)*0*0* f_y f_{y'} I I  \\big) \\bigg) $ \n",
    "$ = E\\big( \\sum \\limits_{y=y'} f^2 I^2_{xy} + 0 \\big)  = $  \n",
    "$ = \\sum limits_{y \\neq y'} f^2_y E(I^2_{xy})= $   \n",
    "$ = <F_2 * \\frac{1}{w} = \\frac{F_2}{w}$    \n",
    "\n",
    "Threfore:\n",
    "$ Var\\big( r(x)c_{H(x)}\\big) \\leq \\frac{F_2}{w}$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## AMS Sampling\n",
    "$ X_1,x_2, ... $\n",
    "$ X = m\\big( g(r)-g(r-1) \\big) $\n",
    "\n",
    "$ E(x|x_J=i) = $  \n",
    "$ \\space \\space \\space = \\sum limits_{k \\in X} k pr(X=k) = $  \n",
    "$ \\space \\space \\space = \\sum \\limits_{k} \\bigg( m\\big( g(r)-g(r-1) \\big) \\bigg) = $  \n",
    "but summing over k's can be simulated by summing over r's\n",
    "$ \\space \\space \\space = \\sum \\limits_{r} \\bigg( m\\big( g(r)-g(r-1) \\big) \\bigg) = $  \n",
    "$ \\space \\space \\space = \\sum \\limits_{r}^{f_i} \\bigg( m\\big( g(r)-g(r-1) \\big) \\bigg) \\frac{1}{f_i} = $  \n",
    "\n",
    "for $k^{th}$ frequency moment :\n",
    "$E(X) = \\sum \\limits_{i}g(f_i) = \\sum \\limits_{i} f^k_i = F_k$\n",
    "\n",
    "Use AMS estimator with $X=m\\big( r^k-(r-1)^k \\big)$\n",
    "gives us a genuine unbiased estimator for any frequency moment (Just keep a small counter for some elements).\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## Dimentionality Reduction\n",
    "\n",
    "Johnson-Lindenstrauss (JL) lemma - a probabilistic approach  \n",
    "\n",
    "$ d \\space \\space \\space X:(...) \\space \\in \\space R^d $  \n",
    "$ d' \\space \\space \\space X':\\phi (x) = \\Pi x \\in R^{d'}$  \n",
    "$ d' < d $ \n",
    "\n",
    "Distances in d' will be similar to distances in d, up to $\\epsilon$ and with $\\delta$ probability (usually deal with Euclidian distances, although it can be general idea to other distances).\n",
    "\n",
    "Next time we will make this efficient using the sparse JL matrix.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
