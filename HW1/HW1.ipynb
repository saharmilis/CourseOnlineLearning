{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Streaming & Online Learning - HW1\n",
    "by Millis Sahar, ID 300420379"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Approximate Counter \n",
    "In our first class, we learned about approximate counting with 𝑂(𝑙𝑜𝑔 𝑙𝑜𝑔 𝑛) bits (for fixed\n",
    "estimation error, and fixed failure probability).\n",
    "This was done by keeping a counter X that is incremented, given a new event, only with probability $ \\frac{1}{2^x} $.\n",
    "The estimator 𝐶̂ was taken to be $ 2^x-1 $.\n",
    "Denote the actual value of the counter by C. We showed in class, that 𝐶̂ is an unbiased estimator, namely, 𝐸[𝐶̂ | 𝐶 = 𝑐] = 𝑐 . \n",
    "\n",
    "<br>\n",
    "\n",
    "###### 1. Show that 𝑉𝑎𝑟(𝐶̂ |𝐶 = 𝑐) = 𝑂(𝑐)  \n",
    "Express $2^x$ as a function of $\\overset{∧}{C}$ :\n",
    "$ C = 2^x-1 \\space \\Rightarrow \\space 2^x = C+1 \\space \\Rightarrow \\space 2^{2x} = 1.5C^2 +1.5C+1$ \n",
    "\n",
    "$ \\overset{∧}{C} = 2^x - 1  \\Rightarrow \\overset{∧}{C} +1 = 2^x $  \n",
    "$ (\\overset{∧}{C} +1)^2 = (2^x)^2 \\Rightarrow \\overset{∧}{C^2} + 2\\overset{∧}{C} + 1  = 2^{2x}$\n",
    "\n",
    "<br>\n",
    "Proff:  \n",
    "$ Var(\\overset{∧}{C}) = $  \n",
    "$ = E(\\overset{∧}{C^2}) - E^2(\\overset{∧}{C}) = $  \n",
    "$ = E\\big((2^x-1)^2\\big) - C^2 = $  \n",
    "$ = E\\big(2^{2x}-2*2^x+1\\big) - C^2 = $  \n",
    "$ = E\\big(2^{2x}\\big) -E\\big(2*2^x\\big) + E(1) - C^2 = $   \n",
    "Using the Expression of $2^x$ and $2^{2x}$ shown above  \n",
    "$ = E\\big(1.5\\overset{∧}{C^2} + 1.5\\overset{∧}{C} + 1\\big) -2E\\big(\\overset{∧}{C} +1\\big) + E(1) - C^2 = $  \n",
    "$ = E\\big(1.5\\overset{∧}{C^2}\\big) + 1.5E\\big(\\overset{∧}{C}\\big) + E(1) -2E\\big(\\overset{∧}{C}\\big) -2E(1) + E(1) - C^2 = $   \n",
    "$ = 1.5C^2 + 1.5C -2C - 1 + 1 + C^2 = $  \n",
    "$ = 0.5C^2 - 0.5C = $  \n",
    "$ = O\\big(0.5C^2 - 0.5C\\big) = $  \n",
    "$ = O\\big(C^2\\big)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Assume we want to count events minus anti-events (logins minus logouts, number of people entering minus number of people exiting).  This is also known as the turnstile model.   \n",
    "- John suggests to do this by storing two approximate counters, one for events and the other for non-events (and then subtracting their estimates).   \n",
    "- Elinoy argues that this is a waste of memory, and says “Why do we need 𝑙𝑜𝑔𝑙𝑜𝑔𝑛 bits after seeing 𝑛 events and 𝑛 anti-events, giving a total of 0? Instead, let’s figure out how to run X backwords given an anti-event!!!”\n",
    "\n",
    "a. Show how Elinoy runs X backwards (in other words, show how to update X, given an anti-event, and prove that your result gives an unbiased estimator of the counter).\n",
    "\n",
    "As talked in class, Elinor is right. She could update X with the probability of $\\frac{1}{2^{x-1}}$.  \n",
    "I'll use mathematical induction to prove $E\\big(2^{x_k}\\big) = n-1$ : \n",
    "\n",
    "$ \\quad \\quad \\text{- Initial value: For K=0 and n=2(due to antievent must come after event):} $  \n",
    "$ \\quad \\quad E\\big(2^{x_0}\\big) = 2-1 $    \n",
    "$ \\quad \\quad E(1) = 1 $  \n",
    "$ \\quad \\quad 1 = 1 $  \n",
    "$ \\quad \\quad True!  $  \n",
    "\n",
    "$ \\quad \\quad \\text{- Hypothesis: }$  \n",
    "$ \\quad \\quad \\text{the statement }E\\big(2^{x_k}\\big) = n-1 \\text{ is true for any value n=k} $  \n",
    "\n",
    "$ \\quad \\quad \\text{- Now I'll prove the Hypothesis statement is true for n = k-1:   }E\\big(2^{x_{k-1}}\\big) = n-2 $     \n",
    "\n",
    "$ \\quad \\quad E\\big(2^{x_{k-1}}\\big) = $  \n",
    "$ \\quad \\quad  = \\sum\\limits_{i=0}^{\\infty} E\\big(2^{x_{k-1}} \\space | \\space x_k=i\\big)Pr(x_k=i) = $      \n",
    "$ \\quad \\quad  = \\sum\\limits_{i=0}^{\\infty}\\big( 2^{i-1}\\frac{1}{2^{i-1}} + 2^i(1-\\frac{1}{2^{i-1}})\\big)Pr(x_k=i) = $  \n",
    "$ \\quad \\quad  = \\sum\\limits_{i=0}^{\\infty}\\big( 2^{i-1}\\frac{1}{2^{i-1}} + 2^i(1-\\frac{2}{2^{i}})\\big)Pr(x_k=i) = $      \n",
    "$ \\quad \\quad  = \\sum\\limits_{i=0}^{\\infty}\\big( (1 + 2^i -2) \\big)Pr(x_k=i) = $      \n",
    "$ \\quad \\quad  = \\sum\\limits_{i=0}^{\\infty} (2^i-1) \\big)Pr(x_k=i) = $  \n",
    "$ \\quad \\quad  = \\sum\\limits_{i=0}^{\\infty}2^iPr(x_k=i) - \\sum\\limits_{i=0}^{\\infty}1Pr(x_k=i) = $    \n",
    "$ \\quad \\quad  = E\\big(2^{x_k}\\big) - E\\big(1\\big) = $    \n",
    "$ \\quad \\quad  \\text{by the Hypothesis}  $  \n",
    "$ \\quad \\quad  = (n-1) - 1 = $  \n",
    "$ \\quad \\quad  = n-2 $  \n",
    "$ \\quad \\quad Done!$  \n",
    "\n",
    "    As requested, I'll prove that this is an unbiased estimator of the counter:  \n",
    "$ \\quad \\quad E\\big(\\overset{~}{n}\\big) = $   \n",
    "$ \\quad \\quad = E\\big(2^x+1\\big) = $   \n",
    "$ \\quad \\quad = E\\big(2^x\\big) + E(1) = $   \n",
    "$ \\quad \\quad = (n-1) + 1  = $  \n",
    "$ \\quad \\quad = n $   \n",
    "$ \\quad \\quad Done!$  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "b. What do you do when X goes back to zero?  \n",
    "\n",
    "When X equals to Zero I'll only INCREASE.  \n",
    "Because the   #LOGOUTS ≤ #LOGINS  by definition of the problem.  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "c. Now analyze Elinoy’s claim more carefully: If 𝑛 events arrive, followed by 𝑛 anti-events, did Elinoy really get rid of the 𝑙𝑜𝑔 𝑙𝑜𝑔 𝑛 bits? \n",
    "\n",
    "In that specific case, John and Elinor will use the same space - O(log n).\n",
    "In every case, the #Logouts won't be \"close\" to the #LOGINS Elinor won't be efficient as she claimd to be. \n",
    "But, this is the worst case for Elinor, therefore she will always surpass (or be equl) to John's way.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Assume that instead of advancing X with probability $\\frac{1}{2^x}$ (as we saw in class), we advance it with probability $\\frac{1}{(1+a)^x}$ , where a > 0 is a parameter.\n",
    "\n",
    "<br>\n",
    "\n",
    "a. What should be our new estimator 𝐶̂ for the counter of elements (as a function\n",
    "of X)?  \n",
    "\n",
    "Define the new estimator : $ \\overset{∧}{C} = \\frac{(1+a)^x-1}{a} $\n",
    "\n",
    "I'll use mathematical induction to prove : $E\\big((1+a)^x\\big)=an+1$\n",
    "\n",
    "- Initial value: For K=0 and $x_k$=0:  \n",
    "    $E\\big((1+a)^0\\big) = a*0 + 1 $  \n",
    "    $E(1) = 0+1$  \n",
    "    $1 = 1$  \n",
    "    True!\n",
    "\n",
    "- Hypothesis:  \n",
    "the statement $E\\big((1+a)^x_k\\big)=ak+1$ is true for any value n=k\n",
    "\n",
    "- Now I'll prove the Hypothesis statement is true for n = k+1: $E\\big((1+a)^{x_{k+1}}\\big)=a(k+1)+1$\n",
    "\n",
    "$ E\\big((1+a)^{x_{k+1}}\\big) = $  \n",
    "$ = \\sum\\limits_{i=0}^{\\infty} E\\big((1+a)^{x_k} \\space |\\space x_k=i\\big) Pr(x_k=i) = $   \n",
    "$ = \\sum\\limits_{i=0}^{\\infty} \\bigg( \\frac{1}{(a+1)^i}(a+1)^{i+1} + \\big(1-\\frac{1}{(a+1)^i}\\big)(a+1)^i\\bigg) Pr(x_k=i) = $  \n",
    "$ = \\sum\\limits_{i=0}^{\\infty} \\bigg( (a+1)^{i+1-i} + (a+1)^i -(a+1)^{i-i}\\bigg) Pr(x_k=i) = $   \n",
    "$ = \\sum\\limits_{i=0}^{\\infty} \\bigg( a+1 + (a+1)^i - 1\\bigg) Pr(x_k=i) = $   \n",
    "$ = \\sum\\limits_{i=0}^{\\infty} \\bigg( a+1 + (a+1)^i - 1\\bigg) Pr(x_k=i) = $  \n",
    "$ = \\sum\\limits_{i=0}^{\\infty} \\big( a + (a+1)^i \\big) Pr(x_k=i) = $  \n",
    "$ = \\sum\\limits_{i=0}^{\\infty} a Pr(x_k=i) + \\sum\\limits_{i=0}^{\\infty} (a+1)^i Pr(x_k=i) = $  \n",
    "$ = E\\big(a\\big) + E\\big( (a+1)^{x_k} \\big) = $  \n",
    "by the Hypothesis  \n",
    "$ = a + ak + 1 = $  \n",
    "$ = a(k+1) +1 $  \n",
    "Done!\n",
    "\n",
    "\n",
    "Also, just in case, I'll prove that this is an unbiased estimator of the counter:  \n",
    "$ E\\big(\\overset{∧}{C}\\big) = $  \n",
    "$ = E\\bigg( \\frac{(1+a)^x-1}{a} \\bigg)= $  \n",
    "$ = \\frac{1}{a}E\\big((1+a)^x-1\\big) = $   \n",
    "$ = \\frac{1}{a}E\\big((1+a)^x\\big) - \\frac{1}{a}E\\big(1\\big) = $  \n",
    "$ = \\frac{1}{a} (an+1) - \\frac{1}{a} = $  \n",
    "$ = \\frac{an}{a} + \\frac{1}{a} - \\frac{1}{a} = $   \n",
    "$ = n$  \n",
    "Done!\n",
    "\n",
    "<br><br>\n",
    "\n",
    "b. What should be the value of parameter 𝑎 so that our estimate satisfies\n",
    "|𝐶 − 𝐶̂ | ≤ 𝜀𝐶 with constant probability (say, at least 2/3) (without\n",
    "averaging many estimators)?\n",
    "\n",
    "As this consept shown in class, I'll use Chebyshev's inequality guarantee in order to find the value that a is limited to, with the probability $\\frac{2}{3}$. And I'll start by finding $ Var\\big(\\overset{∧}{C}\\big) $ .\n",
    "\n",
    "$ Var\\big(\\overset{∧}{C}\\big) = $  \n",
    "$ E\\big(\\overset{∧}{C^2}\\big) - E^2\\big(\\overset{∧}{C}\\big) = $  \n",
    "by unbiased estimator(above)   \n",
    "$ = E\\big(\\overset{∧}{C^2}\\big) - n^2 = $  \n",
    "$ = E\\bigg( \\big(\\frac{(1+a)^x-1}{a}\\big)^2 \\bigg) - n^2 = $  \n",
    "$ = E\\bigg( \\frac{1}{a^2} * \\big(  (1+a)^{2x} - 2(1+a)^x +1 \\big) \\bigg) - n^2 = $  \n",
    "$ = \\frac{1}{a^2} E\\bigg( (1+a)^{2x} - 2(1+a)^x +1 \\bigg) - n^2 = $  \n",
    "$ = \\frac{1}{a^2} E\\big( (1+a)^{2x} \\big) - \\frac{1}{a^2}E\\big(2(1+a)^x\\big) + \\frac{1}{a^2}E(1) - n^2 = $  \n",
    "\n",
    "by the calculations above of $E\\big( (1+a)^{x} \\big) = an+1$  and $E\\big( (1+a)^{2x} \\big) = 1.5(an)^2 + 1.5an + 1$  (like in Q1.1)   \n",
    "\n",
    "$ = \\frac{1}{a^2} \\big(1.5(an)^2 + 1.5an + a\\big) - \\frac{2}{a^2}(an+1) + \\frac{1}{a^2}E(1) - n^2 = $  \n",
    "\n",
    "$ = 1.5n^2 + 1.5\\frac{n}{a} + \\frac{1}{a} + \\frac{2n}{a^2} - \\frac{2}{a^2} + \\frac{1}{a^2} -n^2 = $  \n",
    "$ = 0.5n^2 + \\frac{1.5n+1}{a} + \\frac{2n-1}{a^2} \\leq $   \n",
    "And due to a>0 :   \n",
    "$ \\leq O\\big(n^2\\big) \\leq O\\big(an^2\\big) \\Rightarrow Var(x) \\leq O\\big( an^2 \\big) $\n",
    "\n",
    "And with Chebyshev's inequality guarantee:\n",
    "\n",
    "$\\frac{Var(x)}{\\epsilon^2 n^2} \\leq \\frac{1}{3}$  \n",
    "$\\frac{O\\big( an^2 \\big)}{\\epsilon^2 n^2} \\leq \\frac{1}{3}$  \n",
    "$\\frac{0.5an^2}{\\epsilon^2 n^2} \\leq \\frac{1}{3}$  \n",
    "$\\frac{an^2}{\\epsilon^2 n^2} \\leq \\frac{2}{3}$  \n",
    "$\\frac{a}{\\epsilon^2} \\leq \\frac{2}{3}$  \n",
    "$ a \\leq \\frac{2\\epsilon^2}{3}$  \n",
    "Done!\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "c. Derive a bound on the expected amount of space that such a scheme requires after approximately counting C events.\n",
    "\n",
    "After C events, meaning n=C :  \n",
    "$ (1+a)^{x_C} = aC + 1 $  \n",
    "$ \\log_{1+a}{(1+a)^{x_C}} = \\log_{1+a}{(aC+1)}$  \n",
    "$ x = \\log_{1+a}{aC+1} $   \n",
    "\n",
    "And for amount of space in bits:  $ \\log{\\log_{1+a}{(aC+1)}}$  \n",
    "And by the bound I'll need number of bits $\\leq \\log{\\log_{1+\\frac{2\\epsilon^2}{3}}{(\\frac{2\\epsilon^2C}{3})}}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2 - 𝐹0 Estimator\n",
    "In class we saw Flajolet-Martin estimator for the number of distinct elements 𝐹0\n",
    "frequency\n",
    "moment).   \n",
    "The algorithms we discussed use an “idealized” hash functions that map each element to a floating point in [0,1] with theoretically infinite number of bits.  \n",
    "We will now see an example for another estimator which is using a “realistic” hash function.  \n",
    "Consider the following estimator for 𝐹̂0 (originally from1).  \n",
    "Assume for convenience that $𝑚 = 2^𝑑$ for some d > 0:   \n",
    "\n",
    "• Pick a random hash function ℎ ∶ 𝑆𝑡𝑟𝑒𝑎𝑚 → {0, 1}$^d$ . Namely, we map each element to a 𝑑 dimensional binary vector.  \n",
    "• For each element in the stream, 𝑎, define 𝑟(𝑎) = 𝑚𝑎𝑥{𝑗 ∈ [𝑑] | ℎ(𝑎)1 = · · · = ℎ(𝑎)𝑗 = 0} .  \n",
    "• Let 𝑅 is max(𝑟(𝑎)) for 𝑎∈𝑆𝑡𝑟𝑒𝑎𝑚 (easy to translate this to a pseudocode)  \n",
    "• Estimate the number of distinct elements as 𝐹̂0 = $2^𝑅$ .\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. Ignoring the space required for holding the hash function ℎ, how much space is required for running this algorithm? Explain. (Assume all counters are accurate, and that we’re running only one instance of the algorithm).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ O_{space} (Estimator) = \\sum Space_{Each \\space step} = $  \n",
    "$  = Space_{Hash Functions} + Space_{r(a)} + Space_{R} + Space_{\\overset{∧}{F_0}} = $   \n",
    "Ignoring the space of the Hash functions  \n",
    "$ = 0 + Space_{r(a)} + Space_{R} + Space_{\\overset{∧}{F_0}} = $  \n",
    "For the current element a comming from the stream:  \n",
    "$ = 0 + log(d) + + Space_{R} + Space_{\\overset{∧}{F_0}} = $  \n",
    "Saving R as the maximun of r(a) until this point  \n",
    "$ = 0 + log(d) + + log(d) + Space_{\\overset{∧}{F_0}} = $  \n",
    "Saving $\\overset{∧}{F_0}$ as the predicted estemation   \n",
    "$ = 0 + log(d) + + log(d) + log(n) = $   \n",
    "$ = 2long(d) + log(n) \\leq O\\big(2log(d)\\big) \\leq O\\big(log(d)\\big) \\space or \\space O\\big(loglog(m)\\big) \\space of \\space bits$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. For any element 𝑎 such that his histogram 𝑓𝑎 > 0, and any s ≥ 0, let 𝑊𝑠,𝑎 be the indicator random variable that is equal to 1 if and only if 𝑟(𝑎) > 𝑠, namely: \n",
    "\n",
    "\\begin{equation}\n",
    "\\\\𝑊_{𝑠,𝑎} =\n",
    "\\begin{cases}\n",
    "      1 & s \\leq r(a)   \\\\\n",
    "      0 & r(a) < s \n",
    "\\end{cases} \\space and \\space let \\space Z_s = \\sum \\limits_{a:f_a>0} 𝑊_{𝑠,𝑎}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. What is the variance and expectation of 𝑊𝑠,𝑎?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ E\\big( W_{s,a} \\big) = $  \n",
    "$ = 1*Pr\\big(s \\leq r(a)\\big) + 0*Pr\\big(r(a)<s\\big) = $  \n",
    "$ = Pr\\big(s \\leq r(a)\\big) + 0 = $  \n",
    "$ = Pr\\big(s \\leq r(a)\\big) = $  \n",
    "$ = 0.5^s $  \n",
    "\n",
    "<br>\n",
    "\n",
    "$ Var\\big( W_{s,a} \\big) = $\n",
    "$ = E\\big( W_{s,a}^2 \\big) - E^2\\big( W_{s,a} \\big) = $  \n",
    "$ = 1^2*Pr\\big(s \\leq r(a)\\big) + 0^2*Pr\\big(r(a)<s\\big) - \\big(0.5^2\\big)^2 = $   \n",
    "$ = 0.5^s + 0 - 0.5^{2s} = $  \n",
    "$ = \\frac{1}{2^s} - \\frac{1}{2^{2s}}  = $  \n",
    "$ = \\frac{2^s - 1}{2^{2s}} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. What is the variance and expectation of 𝑍𝑠?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Z_s = \\sum\\limits_{f_a}W_{s,a} $\n",
    "\n",
    "\n",
    "$ E\\big(Z_s\\big) = F_0 * E\\big(W_{s,a}\\big) = F_0 * 0.5^s = \\frac{F_0}{2^s}$\n",
    "\n",
    "Easy to distinguish, that this is a Binomial distribution.\n",
    "Due to the independence of each element a in the stream, and also the independence of the 'seccess' case $ s\\leq r(a) \\big) $ .  \n",
    "From defenition of X~Bin(n,p) $\\Rightarrow$ $Var\\big(X\\big) = np(1-p)$ :\n",
    "\n",
    "$ Var(Z_s) = F_0 * 0.5^s *\\big(1-0.5^s\\big) = F_0 \\frac{2^s-1}{2^{2s}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c. Now notice that R equals exactly the maximal s such that 𝑍𝑠 > 0. Use all this to prove that, if 𝑐$2^𝑠$ < 𝐹0 for some integer s, then Pr(𝑍𝑠 = 0) < $\\frac{1}{c}$    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Pr(Z_s = 0) < \\frac{1}{c} $  \n",
    "I'll use Chebyshev:  \n",
    "$ Pr(Z_s = 0) = \\frac{Var(Z_s)}{E^2(Z_s)} = \\frac{F_0 \\frac{2^s-1}{2^{2s}}}{\\big(\\frac{F_0}{2^s}\\big)^2} = \\frac{F_0(2^s-1)}{F_0^2} = \\frac{2^s-1}{F_0} \\leq \\frac{2^2}{F_0} \\leq $    \n",
    "Using $c2^s < F_0 \\space \\Rightarrow \\space 2^s < \\frac{F_0}{c} $ :  \n",
    "$ \\leq \\frac{\\frac{F_0}{c}}{F_0} = \\frac{1}{c} $  \n",
    "Done!\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d. Conclude that $F_0$ can be smaller than $\\frac{F_0}{c}$ with probability at most $\\frac{1}{c}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, I'll need to prove $P\\big(\\overset{∧}{F_0} < \\frac{F_0}{c}\\big) \\leq \\frac{1}{c}$  \n",
    "Given that $\\overset{∧}{F_0} = 2^R$ , I'll prove $P\\big(2^R < \\frac{F_0}{c}\\big) \\leq \\frac{1}{c}$ which is obviosly TRUE, due to the proof above stating that this $Pr\\big(Z_s=0\\big) < \\frac{1}{c}$ is always true for R which was the maximal s. Therefore the probability will be smaller for any smaller number s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3 - 𝐹2 Estimator\n",
    "\n",
    "The 2nd frequency moment is denoted $𝐹_2$, defined as ∑ $𝑓_𝑎^2$ 𝑎∈𝑆𝑡𝑟𝑒𝑎𝑚 , where $𝑓_𝑎$ denotes the histogram of element a.   \n",
    "\n",
    "We will learn in next lecture the following estimator 𝐹̂2,defined as [∑𝑎∈𝑆𝑡𝑟𝑒𝑎𝑚 𝑠(𝑎)𝑓𝑎 ]$^2$ , where 𝑠 ∶ 𝑆𝑡𝑟𝑒𝑎𝑚 → {±1} is a random hash function (more precisely, we will only 4-wise independent is needed) mapping each element in the steam to 1 or -1.   \n",
    "This estimator is implemented simply by maintaining a counter C, initializing it to 0, and adding 𝑠(𝑎) upon arrival of new stream element a, and finally estimating 𝐹̂$_2$ = 𝐶^2 . \n",
    "\n",
    "Using the usual trick, we will show in class that averaging $𝑂\\big(\\frac{1}{\\epsilon^2}\\big)$ such independent\n",
    "estimators, then taking median of 𝑂(𝑙𝑜𝑔$\\frac{1}{𝛿}$) of independent averages gives an estimator within\n",
    "relative error ε with probability at least 1 − δ.\n",
    "\n",
    "Note that, upon receiving element a, the final estimator (after taking the median of the averaging) requires updating $𝑶\\big(\\frac{𝒍𝒐𝒈(\\frac{1}{𝜹})}{\\epsilon^2}\\big)$ counters.   \n",
    "We will now try to improve this.\n",
    "\n",
    "<br>\n",
    "\n",
    "Consider the following estimator instead:  \n",
    "• As before, choose a random 𝑠 ∶ 𝑆𝑡𝑟𝑒𝑎𝑚 → {±1} together with a random hash ℎ ∶ 𝑆𝑡𝑟𝑒𝑎𝑚 → [𝑘], where k is a parameter (recall that [k]={1,2,…,k})    \n",
    "• Initialize k counters 𝐶1 ,… ,𝐶𝑘.  \n",
    "• Upon arrival of element 𝑎𝑖 , perform the step: 𝐶ℎ(𝑎𝑖) ← 𝐶ℎ(𝑎𝑖) + 𝑠(𝑎𝑖).  \n",
    "• When the stream ends, the estimator is 𝐹̂2 = $\\sum \\limits_{j=1}^{k}C^2_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Prove that 𝑃𝑟[|𝐹̂2 − 𝐹2| > 𝜖𝐹2] ≤13 as long as 𝑘 = 𝑂( $\\frac{1}{\\epsilon^2}$). You can make any randomness assumptions you want about the functions h and s.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll prove with chebyshev that $Pr(|\\overset{∧}{F_2} - F_2|>\\epsilon F_2) \\leq \\frac{1}{3} \\space while \\space k=O\\big(\\frac{1}{\\epsilon^2}\\big)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ E\\big(\\overset{∧}{F_2}\\big) = $  \n",
    "$ = E\\big(\\sum \\limits_{j=1}^{k} C_j^2\\big) = $  \n",
    "by expectation linearity  \n",
    "$ = \\sum \\limits_{j=1}^{k} E\\big( C_j^2\\big) = $  \n",
    "$ = \\sum \\limits_{j=1}^{k} \\bigg[ \\sum \\limits_{i=1}^{n} \\bigg(s(a_i)Pr\\big(h(a_i)=j\\big)\\bigg)^2  \\bigg] = $  \n",
    "$ = \\sum \\limits_{i=1}^{n} \\bigg[ \\sum \\limits_{j=1}^{k} \\bigg(s(a_i)^2Pr^2\\big(h(a_i)=j\\big)\\bigg)  \\bigg] = $  \n",
    "$ = \\sum \\limits_{i=1}^{n} \\bigg[ k*s^2(a_i)\\frac{1}{k}  \\bigg] = $  \n",
    "$ = \\sum \\limits_{i=1}^{n} s^2(a_i) = $  \n",
    "$ = \\sum \\limits_{i=1}^{n} F^2_{a_i} = $  \n",
    "by moments definition  \n",
    "$ = F_2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the complexity of the $Var()$'s calculation, I won't use Chebyshev, and use Chernoff bound:\n",
    "\n",
    "$Pr\\bigg(\\delta E(x) \\leq \\big|X-E(x)\\big|\\bigg) \\leq 2e^{\\frac{-E(x)\\delta^2}{3}} $\n",
    "\n",
    "but In order to do that I'll need to redefine the variables.\n",
    "\n",
    "Let $x_1,x_2 ... x_k$ be independent and identically distributed random variables with range between 0 and 1, and expectation E(x).\n",
    "\n",
    "so $x = \\frac{\\overset{∧}{F_2}}{F_2} $  and $X = \\sum \\limits_{i=1}^{k}x_i = \\sum \\limits_{i=1}^{k}\\frac{C_i^2}{F_2} $ \n",
    "\n",
    "Also,  \n",
    "$E\\big( X \\big) = E\\big(\\sum \\limits_{i=1}^{k}x_i\\big) = E\\big(\\sum \\limits_{i=1}^{k}\\frac{C_i^2}{F_2}\\big) =  \\frac{1}{F_2}E\\big(\\sum \\limits_{i=1}^{k}C_i^2\\big) =   \\frac{1}{F_2}E\\big(\\overset{∧}{F_2}\\big) = \\frac{1}{F_2} * F_2 = 1 $\n",
    "\n",
    "Which means I could use Chernoff with $0<\\epsilon<1$ as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Pr\\bigg(\\epsilon E(x) \\leq \\big|X-E(x)\\big|\\bigg) = $  \n",
    "$ = Pr\\bigg(\\epsilon*1 \\leq \\big|X-1\\big|\\bigg) = $  \n",
    "$ = Pr\\bigg(\\epsilon \\leq \\big|X-1\\big|\\bigg) = $  \n",
    "$ = Pr\\bigg(\\epsilon F_2 \\leq \\big|XF_2-1*F_2\\big|\\bigg) = $  \n",
    "$ = Pr\\bigg(\\epsilon F_2 \\leq \\big|XF_2-F_2\\big|\\bigg) = $  \n",
    "$ = Pr\\bigg(\\epsilon F_2 \\leq \\big|E(\\overset{∧}{F_2})-f_2\\big|\\bigg) = $  \n",
    "$ = Pr\\bigg(\\epsilon E(\\overset{∧}{F_2}) \\leq \\big|E(\\overset{∧}{F_2})-F_2\\big|\\bigg) \\leq $  \n",
    "Choose $ k = \\frac{9}{\\epsilon^2} \\leq O\\big(\\frac{1}{\\epsilon^2}\\big) $  \n",
    "$ \\leq 2e^{\\frac{-1k\\epsilon^2}{3}} \\leq 2e^{\\frac{-\\frac{9}{\\epsilon^2}\\epsilon^2}{3}} \\leq  2e^{\\frac{-3\\epsilon^2}{\\epsilon^2}} \\leq   2e^{-3} \\leq  \\frac{2}{e^3} \\leq \\frac{1}{3} $   \n",
    "Done!!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What is the update time now, per stream element? \n",
    "\n",
    "All the oparations for every new elemnt coming for the Stream, is done by O(1) time.  \n",
    "So the update time of one element is O(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On a persenal note: \n",
    "I had a really hard time solving few of the questions, and I got help from Aviv a number of time.  \n",
    "Also, we had a study group in which we shared ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.1 NOTE:\n",
    "This is a proof I found online, and tried to follow it through, but it is so different then mine, that I added it too.\n",
    "Used it while I wanted to use the Variance in the bound. but it was too hard to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ E\\big( \\overset{∧}{F_2} \\big) = $  \n",
    "$ = E\\big(\\sum \\limits_{j=1}^{k} C_j^2\\big) = $  \n",
    "$ = \\sum \\limits_{j=1}^{k} E\\big(C_j^2\\big) = $  \n",
    "$ = \\sum \\limits_{j=1}^{k}E \\bigg[ \\sum \\limits_{a \\in Stream}\\bigg( f_aS(a)Pr\\big(h(a)=j\\big)  \\bigg)^2 \\bigg] = $  \n",
    "$ =  \\sum \\limits_{j=1}^{k}  E\\bigg[ \\sum \\limits_{a=a}\\bigg( f_aS(a)Pr\\big(h(a)=j\\big)\\bigg)^2 + \\sum \\limits_{a\\neq a'} f_aS(a)Pr\\big(h(a)=j\\big)*f_{a'}S(a')Pr\\big(h(a')=j\\big)   \\bigg] = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k} E\\bigg[ \\sum \\limits_{a=a}\\bigg( f_aS(a)Pr\\big(h(a)=j\\big)\\bigg)^2 \\bigg] +   \\sum \\limits_{j=1}^{k} E\\bigg[ \\sum \\limits_{a\\neq a'} f_aS(a)Pr\\big(h(a)=j\\big)*f_{a'}S(a')Pr\\big(h(a')=j\\big)   \\bigg] = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k}  \\sum \\limits_{a=a} E\\bigg[ \\bigg( f_aS(a)Pr\\big(h(a)=j\\big)\\bigg)^2 \\bigg] +   \\sum \\limits_{j=1}^{k}  \\sum \\limits_{a\\neq a'}E\\bigg[  f_aS(a)Pr\\big(h(a)=j\\big)*f_{a'}S(a')Pr\\big(h(a')=j\\big)   \\bigg] = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k} \\sum \\limits_{a=a} f_a^2 E\\bigg(  S(a)^2Pr^2\\big(h(a)=j\\big) \\bigg) + 0 = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k} \\sum \\limits_{a=a} f_a^2 E\\bigg(  S(a)^2\\bigg)E\\bigg(Pr^2\\big(h(a)=j\\big) \\bigg) = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k} \\sum \\limits_{a=a}f_a^2* 1*E\\bigg(Pr^2\\big(h(a)=j\\big) \\bigg) = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k} \\sum \\limits_{a=a} f_a^2\\bigg(0*Pr^2\\big(h(a)=0\\big)\\bigg)\\bigg(1*Pr^2\\big(h(a)=1\\big)\\bigg) = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k} \\sum \\limits_{a=a} f_a^2 * \\bigg( 1 * (\\frac{1}{k}) \\bigg) = $  \n",
    "$ =   \\sum \\limits_{j=1}^{k} \\sum \\limits_{a=a} f_a^2 \\frac{1}{k} = \\sum \\limits_{j=1}^{k} \\frac{F_2}{k} = k \\frac{F_2}{k}  = F_2 $\n",
    "Done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ E\\big(\\overset{∧}{F_2^2}\\big) = $  \n",
    "$ = E\\bigg( (\\sum \\limits_{j=1}^{k} C_j^2)^2 \\bigg) = $  \n",
    "$ = E\\bigg( \\sum \\limits_{j=1}^{k} C_j^2 * \\sum \\limits_{j=1}^{k} C_j^2 \\bigg) = $  \n",
    "As shown in class, I will Calculate for the 4 cases, but just 2 won't be Zero! when 2 pairs are equal or when all are equal.\n",
    "$ = E\\bigg( \\sum \\limits_{j=1}^{k} C_j^4 \\bigg) + E\\bigg(6\\sum \\limits_{j=1}^{k}\\sum\\limits_{i=1}^{n} C_j^2C_i^2 \\bigg) = $  \n",
    "by expectation linearity  \n",
    "$ = \\sum \\limits_{j=1}^{k} E\\big(  C_j^4 \\big) + 6E\\bigg(\\sum \\limits_{j=1}^{k}\\sum\\limits_{i=1}^{n} C_j^2C_i^2 \\bigg) = $  \n",
    "and by moments definition (and from class and help online)\n",
    "$ = F_4 + \\frac{F^2_2}{k^2}-2F_4 = \\frac{F^2_2}{k^2}-F_4$\n",
    "\n",
    "\n",
    "$ = F_4 - 2(F_2^2 - 2 + F_4^2) - F_2^2 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ Var\\big(F_2\\big) = $   \n",
    "$ = E\\big(F_2^2\\big) - E^2\\big(F_2\\big) = $   \n",
    "$ = \\frac{F^2_2}{k^2}-F_4 - F_2^2 \\leq \\frac{F^2_2}{k^2} - F_2^2 \\leq \\frac{F^2_2 - k^2F_2^2}{k^2}  \\leq \\frac{F^2_2 - k^2F_2^2}{k^2}  $   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Pr\\bigg(a \\leq \\big|X-E(x)\\big|\\bigg) \\leq \\frac{var(x)}{a^2}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
